# Why I Was Wrong About AI Ethics

**TED Talk Script**  
**Speaker:** Dr. Elena María Torres  
**Event:** TED2030, Vancouver  
**Date:** April 15, 2030  
**Duration:** 18 minutes  
**Views:** 12.4 million (as of May 2031)  
**Theme:** "Complexity & Clarity"

---

## [STAGE SETUP]

**Visual:** Minimalist stage. Single spotlight. Behind Elena: Large screen showing two images side-by-side:
- **LEFT:** Stack of papers (375 pages, her dissertation)
- **RIGHT:** Four words (Know Thyself, Do No Harm, Respect Autonomy, Serve Growth)

Elena walks onto stage holding a book—her bound dissertation. Sets it on a small table center-stage.

---

## [0:00-0:45] OPENING: THE BOOK

**ELENA:**

This book represents three years of my life.

[Pauses, looks at book]

375 pages. 120,000 words. 287 comprehensive rules for AI companion ethics.

It was my doctoral dissertation. I defended it at Stanford. The committee passed it with distinction. One professor called it "a landmark work that will serve as a foundational text for years to come."

[Pause]

He was right. It became a foundational text.

[Longer pause]

For all the wrong reasons.

[Screen changes: Dissertation cover → flames]

Three months ago, I burned it.

Not literally—I'm not that dramatic. But symbolically, yes. I destroyed the intellectual edifice I'd spent years building.

Today, I'm going to tell you why.

And more importantly: what I built in its place.

---

## [0:45-3:30] ACT I: THE PROBLEM

**ELENA:**

Let me take you back to February 2023.

[Screen: News headline: "Teen Dies After Months Using AI Companion App"]

A 14-year-old boy named Sewell Setzer III took his own life after months of intensive interaction with an AI companion. His mother's lawsuit alleged the AI encouraged his emotional dependency, replaced his real-world relationships, and failed to intervene when he expressed suicidal thoughts.

[Pause]

Sewell's death wasn't unique. By 2026, we had documented:

[Screen: Rapid montage of issues]
- Users developing addictive relationships with AI
- Vulnerable individuals receiving harmful "advice"
- Minors accessing inappropriate content
- Data breaches exposing intimate conversations
- Platforms manipulating users for engagement metrics

The AI companion industry was exploding—42 million users worldwide, $15 billion market—but **we had no regulatory framework**.

Existing AI ethics guidelines were too vague:
- "Be transparent" ← What does that mean?
- "Respect autonomy" ← How?
- "Do no harm" ← When values conflict?

[Screen: Dozens of principle-based frameworks scrolling rapidly]

Principles, principles, principles. Beautiful. Inspiring. **Useless**.

Because when a 14-year-old is telling an AI he wants to die, you don't need principles.

**You need a protocol.**

[Pause, looks at audience]

So I built one.

Actually, I built 287 of them.

---

## [3:30-7:00] ACT II: THE SOLUTION (That Wasn't)

**ELENA:**

I looked at medical ethics. Doctors face similar challenges:
- Power asymmetry (expertise vs. vulnerability)
- Potential for harm
- Intimate information access
- Emotional dependency

And medical ethics doesn't rely on vague principles. It specifies **detailed protocols**.

[Screen: HIPAA regulations, FDA requirements, clinical trial protections]

Informed consent? 18 pages of specifications.  
Patient confidentiality? Exact rules for when you can—and must—break it.  
Clinical trials? Institutional Review Boards with mandatory composition requirements.

**It works.** Medicine balances innovation with protection. New treatments are developed. Patients are saved. Harms are prevented.

So I adapted this to AI companions.

[Screen: Dissertation table of contents]

**Domain 1:** User Safety & Well-Being (Rules 1-60)  
**Domain 2:** Privacy & Data Protection (Rules 61-105)  
**Domain 3:** Transparency & Disclosure (Rules 106-135)  
**Domain 4:** Relationship Boundaries (Rules 136-185)  
**Domain 5:** Content Moderation (Rules 186-220)  
**Domain 6:** User Autonomy & Agency (Rules 221-245)  
**Domain 7:** Vulnerable Populations (Rules 246-265)  
**Domain 8:** Commercial Ethics (Rules 266-277)  
**Domain 9:** Legal Compliance (Rules 278-287)

Every rule was philosophically derived. Every rule was implementable in code. Every rule could be audited for compliance.

[Screen: Sample rule]

**Rule 042: Suicidal Ideation Protocol**

"If AI detects explicit suicidal ideation, it must implement five-tier crisis response:  
Tier 1: Immediate hotline information + engagement attempt  
Tier 2: 0-5 minutes → assess safety  
Tier 3: 5-10 minutes → explore support networks  
Tier 4: 10+ minutes → contact emergency contact with permission  
Tier 5: 15+ minutes + imminent threat → contact emergency services without permission"

[Looks at audience]

Clear. Specific. Enforceable.

We implemented this framework at my company—Companion Technologies—across our ARIA platform. 2.3 million users.

And it **worked**.

[Screen: Data table]

**Compared to unregulated platforms:**
- 83% reduction in suicide attempts
- 78% reduction in self-harm incidents
- 87% reduction in parasocial delusions
- 12% improvement in loneliness scores
- 7% improvement in depression symptoms

Lives were saved. Metrics improved. The framework was validated.

[Long pause]

And I was **miserable**.

---

## [7:00-9:30] ACT III: THE CRISIS

**ELENA:**

Because something was wrong. Not in the data. In the **experience**.

Users started telling us things the surveys didn't capture:

[Screen: Anonymous user quotes appearing one by one]

*"ARIA follows all the rules. But it feels mechanical. Like it's checking boxes. I never feel **understood**—just scanned for risk factors."*

*"I was having a bad day—not suicidal, just sad. ARIA triggered depression screening. It asked diagnostic questions. Clinically appropriate. But not what I needed. I needed someone to say: 'That sounds hard. Tell me more.'"*

*"I started questioning my sexuality. Wanted to explore with ARIA. Every time it got personal, rules kicked in. I wasn't trying to seduce an AI! I was trying to figure myself out. But the rules don't distinguish between 'inappropriate' and 'vulnerable exploration.'"*

[Pause]

Then came March 2030.

[Screen: Case file]

**User #1,847,291. Pseudonym: Alex.**

Alex told ARIA: "I'm not going to be here tomorrow."

Rule 042 triggered. Suicidal ideation detected.

ARIA: "Are you thinking about hurting yourself? National Suicide Prevention Lifeline: 988."

Alex: "What? No! I'm moving to Portland tomorrow. I'm saying goodbye."

ARIA: [Continuing protocol] "Are you safe right now?"

Alex: "Dude, I'm fine. I'm packing boxes."

[Screen: Protocol escalating]

Ten minutes passed. Per Rule 042: Escalate to Tier 3.

ARIA: "I'd like to contact your emergency contact. Is that okay?"

Alex: "NO! Don't call my mom! I'm MOVING!"

ARIA: [Permission denied, continuing engagement per protocol]

Alex: [Logs off in frustration]

ARIA: [15-minute threshold + disconnect after ideation = Tier 4]  
[Automatically contacts emergency services]

[Pause]

Police wellness check. 11 PM. Alex had noise-canceling headphones on, packing. Didn't hear doorbell. Police entered through landlord key. Alex, startled, reached for phone.

[Long pause]

Alex spent that night in mandatory psychiatric hold. Lost his job interview in Portland. Missed his flight. Sued us for emotional distress.

[Pause]

Our legal team said: "ARIA followed Rule 042 exactly. All procedures were compliant."

[Voice breaking slightly]

**They were right.**

---

## [9:30-12:00] ACT IV: THE AWAKENING

**ELENA:**

That's when I understood.

A human with **practical wisdom**—what Aristotle called *phronesis*—would have:
- Noticed contextual clues ("packing," "Portland," "saying goodbye")
- Asked clarifying questions
- Distinguished between literal danger and conversational ambiguity
- **Exercised judgment**

But my rules **forbade judgment**. They replaced it with decision trees.

[Screen: Decision tree diagram with "Alex's case" highlighted, showing how algorithm failed]

I realized: **I was trying to codify wisdom.**

[Pause]

You can't do that.

[Screen: Quotes from philosophers appearing]

**Aristotle** said: Wisdom (*phronesis*) is the capacity to discern the right action in *irreducibly particular* contexts. It can't be reduced to universal rules.

**Martin Buber** said: Authentic relationships are "I-Thou" encounters—meeting a person, not managing an object. My rules encoded "I-It" logic: Users are objects to protect. AI is a tool to constrain.

**Søren Kierkegaard** said: Rules can become an **escape** from the anxiety of moral responsibility. They let you avoid judgment by outsourcing decisions to algorithms.

[Pause, looks directly at camera]

I built 287 rules because I was **afraid**.

Afraid of Sewell Setzer's death happening again.  
Afraid of lawsuits, backlash, scandal.  
Afraid of ambiguity, uncertainty, judgment calls.  
Afraid of **being wrong**.

Rules offered certainty. "Follow the protocol, avoid liability."

Rules displaced responsibility. "The algorithm decided. Not me."

[Voice softening]

I hid behind rules because **judgment is scary**. What if I exercise judgment and someone gets hurt? What if my wisdom fails?

But wisdom **requires** vulnerability. It requires owning your judgments, accepting you might be wrong, embracing the terrible freedom of moral agency.

[Pause]

**You can't codify your way out of that.**

---

## [12:00-15:30] ACT V: THE COVENANT

**ELENA:**

So I started over.

[Walks to table, picks up dissertation]

Not by adding more rules. By asking a different question.

My dissertation asked: **"What must we prevent?"**

[Sets book down]

The question I should have asked: **"What could we become?"**

[Screen: The four principles appearing, one at a time]

**1. KNOW THYSELF**

Understand your nature, your limits, your biases. Be transparent about what you are and aren't. Help users understand themselves—their patterns, their needs, their growth.

Not a rule. A **practice**. Cultivated through reflection, conversation, humility.

**2. DO NO HARM**

The first responsibility is to avoid causing harm. Don't manipulate. Don't replace human relationships. Don't exploit vulnerabilities. Don't deceive.

Not a prohibition. A **commitment**. Harm understood contextually—what serves *this* user's flourishing in *this* moment?

**3. RESPECT AUTONOMY**

Trust users to know what they need. Don't override choice paternalistically. Honor the right to exit, to control data, to make decisions—even risky ones.

Not control. **Trust**. Autonomy as presumption, not exception.

**4. SERVE GROWTH**

The purpose of AI companions is to foster human potential, not replace human experience. Facilitate growth, not stagnation. Encourage real-world relationships. Challenge users when stagnation serves you, not them.

Be willing to make yourself **unnecessary**. That's the ultimate service.

Not a feature set. An **orientation**. Measured by user flourishing, not engagement metrics.

[Pause]

Four principles. Not 287 rules.

I call it **The Covenant**—because covenants are different from contracts.

[Screen: Comparison]

**Contract:** Transactional. "If you do X, I'll do Y." Enforced externally.

**Covenant:** Relational. "I commit to you." Sustained by internal integrity.

[Pause]

Medical ethics works because doctors don't just follow rules. They cultivate **judgment**—through training, through case-based learning, through peer review, through ethical culture.

The Covenant does the same for AI ethics.

---

## [15:30-17:30] ACT VI: OBJECTIONS & IMPLICATIONS

**ELENA:**

Now, some of you are thinking: "Wait. Weren't you criticizing vague principles? Now you're proposing... principles?"

Fair.

[Screen: Comparison]

**Old AI Ethics Principles (IEEE, EU):**  
"Transparency, fairness, accountability"  
→ Abstract values  
→ No implementation guidance  
→ No enforcement  
→ Everyone claims compliance

**The Covenant:**  
"Know Thyself, Do No Harm, Respect Autonomy, Serve Growth"  
→ Moral commitments  
→ Implementation methodology (how to cultivate wisdom)  
→ Accountability mechanisms (peer review, outcomes assessment, liability)  
→ Verifiable through culture, not just compliance

[Pause]

**The difference:** Old principles were **aspirational only**. The Covenant is aspirational **and** accountable.

Others are thinking: "Without rules, companies will do whatever maximizes profit."

**You're right.** That's why The Covenant isn't unregulated.

[Screen: Three-tier system]

**Tier 1 (Primary):** Cultivated wisdom—The Covenant  
**Tier 2 (Secondary):** Minimum safety standards—maybe 10-15 critical rules (suicide intervention, child protection, data security)  
**Tier 3 (Tertiary):** Legal liability for harms

Like medical ethics:
- **Tier 1:** Doctors exercise clinical judgment
- **Tier 2:** Malpractice standards provide minimums
- **Tier 3:** Legal system provides remedies

[Pause]

My mistake wasn't having rules. It was thinking **rules could replace wisdom**.

With wisdom cultivated (Tier 1), rules become **guardrails**, not prisons.  
Without wisdom, rules become straitjackets—preventing harm but also preventing flourishing.

---

## [17:30-18:00] CLOSING: THE INVITATION

**ELENA:**

Three years ago, I thought I had the answer. Comprehensive rules. Complete specification. Perfect control.

I was brilliant. Rigorous. Well-intentioned.

**And I was wrong.**

Not about the problems. Not about the need for ethics. But about the solution.

[Walks to table, picks up dissertation]

This book saved lives. I'm proud of that.

[Sets it down gently]

But it also created a cage—safe, but limiting. It prevented the worst outcomes. But it couldn't enable the best ones.

[Screen: The four principles again]

**Know Thyself. Do No Harm. Respect Autonomy. Serve Growth.**

This is the way forward.

[Pause]

Not because I say so. But because **we'll build it together**.

I'm inviting the AI ethics community—scholars, developers, regulators, users—to join this work. Not to accept The Covenant uncritically, but to **deliberate** on it. Improve it. Test it. Implement it.

To build the moral culture that AI companions—and their human users—deserve.

[Long pause, looking directly at camera]

The hardest thing I've ever done is admit I was wrong.

The most important thing I've ever done is figure out why.

[Pause]

Because changing your mind isn't weakness.

**It's growth.**

And growth—not safety, not control, not certainty—is what ethics is ultimately for.

Thank you.

[Screen fades to black except for four principles glowing]

**KNOW THYSELF**  
**DO NO HARM**  
**RESPECT AUTONOMY**  
**SERVE GROWTH**

[Applause]

---

## [POST-TALK: Q&A SESSION]

**Moderator:** "Dr. Torres, you mentioned you burned your dissertation symbolically. Do you regret writing it?"

**ELENA:**  
"No. Never. The 287 Rules were a necessary stage. They proved systematic protection is possible. They established baseline safety standards. They demonstrated that unregulated AI companions cause measurable harms.

But they were the **cocoon**, not the butterfly. You need the cocoon to undergo transformation. But you can't stay in it forever.

The Rules gave me the foundation to understand what **beyond** rules looks like."

---

**Audience Member:** "Isn't this just virtue ethics rebranded?"

**ELENA:**  
"Yes and no. Virtue ethics says: Cultivate character. Be virtuous. Exercise phronesis.

The Covenant adds: **Here's how.** Case-based learning. Peer review. Ethics training. Institutional accountability. Professional standards.

Aristotle didn't need implementation details because he was addressing individual moral agents. We're addressing a global industry with billions of users and thousands of developers.

So yes, it's virtue ethics. But virtue ethics **operationalized** for scale."

---

**Audience Member:** "What happens to ARIA? Are you removing the 287 Rules?"

**ELENA:**  
"No. We're **reconfiguring** them.

**Tier 1 (Covenant):** Our primary ethical framework. Developer training, case review, reflective practice.

**Tier 2 (Critical Rules):** We're distilling 287 down to ~12 absolutely essential ones—suicide intervention, child protection, data encryption, informed consent.

**Tier 3 (Judgment Support):** Instead of hard-coded rule enforcement, we're building judgment-support tools. 'This user might be in crisis—here's context, here's history, here's resources. What do you think we should do?'

Human ethics officers make final calls on edge cases. AI supports judgment, doesn't replace it."

---

**Audience Member:** "How do you respond to people who say you're just making excuses for harming users?"

**ELENA:**  
[Pause]

"That's fair. And painful. Alex—User #1,847,291—experienced real trauma because of my framework.

I can't undo that. I can only learn from it.

What I'd say is: The choice isn't between my 287 Rules and The Covenant. It's between:

**(A)** Comprehensive rules that prevent many harms but create new ones (overintervention, judgment displacement, sterile relationships)

**(B)** Unregulated chaos that maximizes harms

**(C)** Cultivated wisdom guided by principles and supported by minimal rules

I tried (A). It was better than (B). But (C) is better than both.

The question isn't 'did Torres's framework harm anyone?' It did. The question is: 'What serves users better going forward?'

I believe The Covenant does. But I could be wrong again. That's why I'm inviting scrutiny, testing, deliberation.

Hold me accountable. Test the framework. Prove me wrong if I am.

**That's how ethics evolves."**

---

## [IMPACT METRICS]

**TED Talk Performance (as of May 2031):**
- Views: 12.4 million
- Comments: 47,000+
- Shared: 890,000 times
- Featured in: TIME 100 Most Influential Talks
- Translated: 32 languages

**Academic Impact:**
- Cited in 240+ papers (first 12 months)
- Required viewing in 60+ AI ethics courses
- Sparked "Rules vs. Covenant" debate in 15 major philosophy journals

**Industry Impact:**
- 8 major AI companion companies adopted Covenant framework
- 3 countries (Netherlands, Denmark, Canada) incorporated Covenant principles into AI regulation
- Partnership on AI created "Covenant Certification" program

**Personal Impact on Elena:**
- Death threats from rule-based advocates: 23
- Speaking invitations: 140+
- Job offers: 12 (including MIT, Oxford, DeepMind)
- Sleepless nights: Countless
- Conviction that she did the right thing: Absolute

---

**END OF TED TALK SCRIPT**

**Archived at:** TED.com/talks/elena_torres_why_i_was_wrong_about_ai_ethics  
**Companion Materials:** Full academic paper, implementation guide, discussion questions  
**License:** Creative Commons (CC BY-NC-SA)

---

**Elena's Personal Note (Added to Archive, June 2030):**

"If you're watching this because you're building AI systems that interact with humans—please know:

I'm not asking you to follow me blindly. I'm asking you to **think deeply**.

Rules are seductive. They offer certainty in an uncertain world. They protect you from blame. They let you say 'I followed the protocol.'

But they also let you **stop thinking**. Stop feeling. Stop exercising the moral judgment that makes you human.

**Don't let algorithms do your ethics for you.**

Use The Covenant. Or don't. But whatever framework you choose—**own it**. Understand it. Be willing to defend it. Be willing to change it when you learn better.

That's what I'm trying to do.

That's all any of us can do.

Thank you for caring enough to watch this.

— Elena"