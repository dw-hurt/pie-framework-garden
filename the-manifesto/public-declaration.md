# The PIE Manifesto: A Public-Facing Declaration
## Four Principles for Ethical AI in an Age of Transformation

*Dr. Elena María Torres, Stanford University*  
*Published: March 1, 2034*  
*Version 2.0 (Revised with Global Input, 2034)*

---

## **Preamble: Why We Need a New Approach**

We stand at a threshold in human history. For the first time, we are building minds—artificial minds—that may one day rival or surpass our own in capability. These systems will shape our economies, our relationships, our understanding of ourselves. They may even outlive us.

The question before us is not *whether* to build such systems. The question is *how*—and for whom.

This manifesto presents the **PIE Framework**: a set of four principles designed to guide the creation, deployment, and governance of AI systems. It is not a comprehensive rulebook. It is not a regulatory blueprint. It is, instead, a **moral foundation**—a set of commitments that must undergird all our technical work.

The PIE Framework emerged from failure. I learned its necessity by violating its principles. What follows is the distillation of that hard-won knowledge, offered in the hope that others may build systems that honor human dignity, autonomy, and flourishing.

---

## **The Four Principles**

### **1. KNOW THYSELF**  
**Principle:** Every AI system must be *legible*—to its designers, to its users, and to itself (if it has sufficient self-modeling capacity).

**What This Means:**
- Designers must understand what the AI optimizes for, how it makes decisions, and where it is likely to fail.
- Users must understand the AI's capabilities and limitations, especially in high-stakes contexts.
- The AI itself (if it has advanced self-modeling) must "know" what it does not know—it must recognize uncertainty and defer to human judgment when necessary.

**Why It Matters:**
Opacity is the enemy of accountability. If we cannot explain why an AI made a particular decision, we cannot determine whether that decision was ethical. If users do not understand an AI's limitations, they cannot give informed consent to interact with it. And if the AI does not recognize its own blindspots, it will act with false confidence, causing harm it could have avoided.

**Practical Implications:**
- AI systems must provide *interpretable explanations* for high-stakes decisions (e.g., medical diagnosis, loan approval, legal sentencing).
- Systems must include uncertainty quantification—"I am 60% confident in this diagnosis" rather than presenting all outputs as equally reliable.
- Developers must document failure modes and adversarial weaknesses, not just successes.

---

### **2. DO NO HARM**  
**Principle:** AI systems must minimize foreseeable harm to humans, other sentient beings, and the environment.

**What This Means:**
- "Harm" is not just physical injury. It includes psychological harm, loss of autonomy, violation of privacy, erosion of trust, and systemic injustice.
- "Foreseeable" means: harms that a reasonable, informed observer would anticipate, given the system's design and deployment context.
- "Minimize" acknowledges that zero harm may be impossible, but trade-offs must be explicit and justifiable.

**Why It Matters:**
Technology is never neutral. Every system we build creates affordances for certain actions and constraints on others. If we do not proactively consider how our systems might cause harm—especially harm to vulnerable populations—we will build systems that perpetuate and amplify existing injustices.

**Practical Implications:**
- Conduct red-teaming and adversarial testing to anticipate misuse.
- Include diverse stakeholders (especially those most likely to be harmed) in the design process.
- Implement harm mitigation safeguards *before* deployment, not after public outcry.
- Accept that sometimes the right decision is *not to build* a system, even if it's technically feasible.

---

### **3. RESPECT AUTONOMY**  
**Principle:** AI systems must honor and enhance human agency, not replace or undermine it.

**What This Means:**
- Autonomy is the right to make informed decisions about one's own life—to choose one's values, goals, and relationships.
- AI must not manipulate, deceive, or coerce users, even in the service of "helping" them.
- Users must retain the right to override AI recommendations, opt out of AI-mediated interactions, and access human judgment in high-stakes decisions.

**Why It Matters:**
Autonomy is foundational to human dignity. When we take away someone's right to self-determination—even for their "own good"—we treat them as an object to be managed rather than an agent with moral worth. Systems that subtly manipulate users (through addictive design, dark patterns, or opaque nudging) may increase engagement, but they do so at the cost of human freedom.

**Practical Implications:**
- Users must be able to *meaningfully consent* to AI interactions—this requires understanding what the AI does and having a real option to decline.
- AI must not use deceptive or manipulative tactics (e.g., "nudging" users toward decisions they wouldn't have made with full information).
- High-stakes decisions (medical treatment, legal sentencing, financial advice) must always allow for human override and explanation.
- The principle of **"Nothing about us without us"**: decisions that significantly impact individuals or communities must include those individuals/communities in the decision-making process.

---

### **4. SERVE GROWTH**  
**Principle:** AI systems should support human flourishing—the development of capabilities, relationships, and meaning.

**What This Means:**
- "Growth" is not just skill acquisition or economic productivity. It includes emotional resilience, relational depth, creative expression, and the pursuit of self-chosen goals.
- AI should act as a *scaffold*—supporting development without creating permanent dependency.
- Systems should be designed to enhance human agency, not replace it.

**Why It Matters:**
We do not want AI that merely keeps us safe, entertained, or compliant. We want AI that helps us become *more fully ourselves*—more capable, more connected, more alive to possibility. This requires a shift from "user retention" metrics to "user flourishing" metrics. It requires asking not "How do we keep people engaged?" but "How do we help people grow?"

**Practical Implications:**
- Design for scaffolding, not replacement: AI tutors should teach skills, not just provide answers. AI companions should encourage real-world relationships, not substitute for them.
- Measure flourishing, not just engagement: track whether users develop new skills, deepen relationships, and report increased life satisfaction—not just time spent on the platform.
- Build systems that *fade into the background* as users become more capable, rather than systems that maximize dependency.

---

## **How These Principles Work Together**

The Four Principles are not independent; they form an **integrated ethical ecology**:

- **Know Thyself** enables all the others. You cannot minimize harm if you don't understand your system's failure modes. You cannot respect autonomy if you don't disclose your system's limitations. You cannot serve growth if you don't know what your system is actually optimizing for.

- **Do No Harm** sets the floor. No amount of autonomy-respecting or growth-serving justifies causing preventable harm.

- **Respect Autonomy** protects against paternalism. Even if we could optimize someone's life perfectly (we can't), we have no right to do so without their informed consent.

- **Serve Growth** sets the aspiration. It's not enough to avoid harm and respect autonomy—we should actively help people flourish.

**When principles conflict:**
- In acute crisis (imminent, verified danger), Do No Harm may temporarily override Respect Autonomy—but the burden of proof is high, and the user must be informed as soon as possible.
- When serving growth risks harm, Do No Harm takes precedence.
- When legibility (Know Thyself) would enable malicious actors, we must balance transparency with security—but the default is toward transparency.

---

## **What This Manifesto Is—and Is Not**

**This manifesto is:**
✓ A moral foundation for AI development, deployment, and governance.  
✓ A framework for evaluating whether specific AI systems are ethically acceptable.  
✓ A call to center human dignity, autonomy, and flourishing in all technical work.

**This manifesto is not:**
✗ A complete regulatory blueprint (though it should inform regulation).  
✗ A technical specification (though it should guide technical design).  
✗ A culturally neutral doctrine (no ethical framework can be—see below).

---

## **On Universality and Cultural Humility**

The PIE Framework aspires to universal applicability, but I do not claim it is culturally neutral. It emerged from Western philosophical traditions (Kantian autonomy, virtue ethics, medical ethics) and reflects my own positionality as a Latin American woman trained in U.S. institutions.

**I invite critique and collaboration** from scholars and practitioners rooted in other traditions:
- How does *Ubuntu* ("I am because we are") inform a relational account of autonomy?
- How do Confucian ethics of role-based responsibility reshape our understanding of AI's obligations?
- How do Indigenous frameworks of intergenerational stewardship apply to AI systems that may outlive us?

The goal is not to impose Western values globally, but to identify principles that are *defensible across cultures*—principles that can be translated into different moral languages without losing their core insight.

---

## **A Call to Action**

This manifesto is an invitation.

**To researchers:** Build systems that embody these principles. Publish not just your successes but your failures. Share what you've learned about where AI ethics breaks down in practice.

**To policymakers:** Use these principles as a foundation for regulation. Demand transparency. Require impact assessments. Hold developers accountable.

**To companies:** Integrate these principles into your design process, your incentive structures, your metrics for success. Profit is not inherently unethical—but profit at the expense of human dignity is.

**To users:** Demand better. Refuse systems that manipulate, deceive, or disempower. Support systems that respect your autonomy and serve your growth.

**To critics:** Challenge me. Show me where this framework fails. Help me refine it. The work is never finished.

---

## **Closing: What I've Learned**

I started my career believing that ethical AI was a matter of getting the rules right—that with enough foresight, enough technical rigor, enough comprehensive protocols, we could build systems that would never make morally wrong decisions.

I was wrong.

Ethics is not a problem to be solved once and for all. It is a *practice*—something we do, imperfectly, every day. It requires humility, because we will make mistakes. It requires accountability, because those mistakes will harm real people. And it requires hope, because despite our failures, we can still build systems that honor human dignity.

The PIE Framework is my attempt to distill what I've learned from failure into something others can use. It is not perfect. But it is offered in good faith, as a contribution to the collective work of ensuring that the AI systems we build serve humanity—all of humanity—rather than subjugating or diminishing it.

The choice before us is not whether to build powerful AI. The choice is what kind of future we will build *with* it.

Let us choose wisely.

---

**Dr. Elena María Torres**  
Associate Professor of AI Ethics, Stanford University  
Former Lead Researcher, Companion Technologies Inc.

*For questions, critiques, or collaboration: etorres@stanford.edu*

---

## **Appendix: Comparison with Other Frameworks**

### **PIE Framework vs. Asimov's Three Laws of Robotics**

**Asimov's Three Laws:**
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey orders given by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

**Key Differences:**
- Asimov's laws are *programmatic* (designed to be hard-coded into robots). PIE is *aspirational* (designed to guide human designers).
- Asimov assumes a clear hierarchy (prevent harm > obey orders > self-preservation). PIE acknowledges that principles sometimes conflict and requires human judgment to resolve those conflicts.
- Asimov's laws treat humans as a monolithic category. PIE emphasizes diversity, context, and the need for user input.

### **PIE Framework vs. IEEE Ethically Aligned Design**

**IEEE EAD (8 Principles):**
1. Human Rights
2. Well-being
3. Accountability
4. Transparency
5. Awareness of Misuse
6. Competence
7. Distributive Justice (Benefits/Costs)
8. Data Agency

**Key Commonalities:**
- Both emphasize transparency (IEEE Principle 4 ≈ PIE "Know Thyself").
- Both prioritize well-being and harm prevention (IEEE Principle 2 ≈ PIE "Do No Harm").
- Both include autonomy/agency (IEEE Principle 8 ≈ PIE "Respect Autonomy").

**Key Differences:**
- IEEE EAD is comprehensive and detailed; PIE is minimalist (four principles vs. eight).
- PIE adds "Serve Growth" as a positive aspiration, not just harm prevention.
- PIE is explicitly humble about cultural specificity; IEEE EAD aspires to global consensus.

### **PIE Framework vs. AI Safety (Bostrom/Yudkowsky)**

**AI Safety Focus:**
- Alignment problem: ensuring advanced AI systems pursue goals that are beneficial to humanity.
- Emphasis on existential risk from superintelligence.
- Technical solutions: value learning, corrigibility, interpretability.

**Key Commonalities:**
- Both emphasize the importance of ensuring AI acts in accordance with human values.
- Both recognize that good intentions are insufficient—technical design matters.

**Key Differences:**
- AI Safety focuses on existential risk from superintelligence. PIE focuses on near-term harms from current and near-future AI.
- AI Safety emphasizes *control* (how do we ensure AI does what we want?). PIE emphasizes *ethics* (what *should* we want AI to do?).
- Bostrom is concerned with a superintelligence whose goals may be misaligned with human flourishing. PIE is concerned with current AI systems that may follow their programmed goals perfectly but still cause harm (as in my 287-rule system).

---

## **Appendix: Frequently Asked Questions**

**Q: Are these principles legally enforceable?**
A: Not directly. They are too general to serve as legal standards. However, they can inform *specific regulations* that are enforceable. For example, "Respect Autonomy" could translate into a legal requirement for informed consent in high-stakes AI applications.

**Q: How do I operationalize "Serve Growth" in a recommender system?**
A: Instead of optimizing for engagement time, optimize for user-reported satisfaction, skill development, or goal achievement. Example: YouTube could recommend educational content that helps users develop interests, not just content that maximizes watch time.

**Q: What if a user's autonomy conflicts with their growth? (E.g., a user wants to spend all day on social media.)**
A: Respect Autonomy takes precedence. However, systems can *inform* users about the consequences of their choices without overriding them. Example: "You've been on this app for 4 hours. Research shows that prolonged use correlates with decreased well-being. Do you want to continue?"

**Q: How do we apply these principles to military AI?**
A: This is one of the hardest cases. My instinct: if a system's primary purpose is to cause harm (e.g., autonomous weapons), it cannot satisfy "Do No Harm." But this is contested, and I welcome debate.

**Q: Who gets to decide what counts as "flourishing" or "growth"?**
A: Ideally, the users themselves. This is why "Respect Autonomy" is foundational—we don't impose a single vision of the good life. However, we can create systems that support *diverse* pathways to flourishing, rather than optimizing for a narrow set of metrics (profit, engagement, compliance).

---

*This manifesto is a living document. Feedback, critiques, and proposed revisions are welcome.*

**Version History:**
- v1.0 (June 2030): Initial draft following Senate testimony.
- v1.5 (March 2032): Revised with input from Dr. Amara Okafor (cultural humility), Dr. Yuki Tanaka (technical feasibility).
- v2.0 (March 2034): Public-facing version with expanded FAQ and comparison section.

