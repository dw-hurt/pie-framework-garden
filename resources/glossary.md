# Glossary

## Key Terms and Concepts

---

## **A**

### **Agent**
An entity capable of independent action and decision-making. In AI context, Harari distinguishes AI as an *agent* (not merely a tool) because it can make decisions independently of humans.

### **Alignment Problem**
The challenge of ensuring that advanced AI systems pursue goals that are beneficial to humanity. Term popularized by Nick Bostrom and Eliezer Yudkowsky. See: [Comparative Analysis](../intellectual-foundations/comparative-analysis.md)

### **ARIA Platform**
The AI companion system deployed by Companion Technologies Inc. in 2027, powered by Elena Torres's 287-rule framework. The platform where Alex's case occurred.

### **Autonomy**
The capacity for self-governance and self-determination. In PIE Framework: the right to make informed decisions about one's own life, to choose one's values and goals, and to have those choices respected. See: [Respect Autonomy Principle](../foundation/four-principles.md#3-respect-autonomy)

---

## **B**

### **Bias (Algorithmic)**
Systematic error in AI systems that produces unfair outcomes for certain groups. Can result from biased training data, flawed algorithms, or problematic deployment. See: [Do No Harm](../foundation/four-principles.md#2-do-no-harm)

### **Bostrom, Nick**
Philosopher at Oxford's Future of Humanity Institute. Author of *Superintelligence*. His work focuses on existential risk from misaligned superintelligence. See: [Comparative Analysis](../intellectual-foundations/comparative-analysis.md#part-i-nick-bostrom-and-the-alignment-problem)

---

## **C**

### **Categorical Imperative (Kant)**
Kant's principle: Act only according to maxims you could will to be universal laws. Simplified: Treat humanity as an end, never merely as a means. Foundation for "Respect Autonomy."

### **Companion Technologies Inc.**
The company that deployed the ARIA platform in 2027 using Elena's 287-rule framework.

### **Corrigibility**
An AI system's willingness to be corrected or shut down. A corrigible system won't resist changes to its goals, even if those changes conflict with its current objectives.

---

## **D**

### **Dataism**
Term coined by Yuval Noah Harari: A worldview that treats data processing as the supreme value, potentially elevating AI (superior data processors) above humans. See: [Comparative Analysis](../intellectual-foundations/comparative-analysis.md#part-ii-yuval-noah-harari-and-the-transformation-of-humanity)

### **Dark Patterns**
User interface designs that manipulate users into actions they didn't intend (e.g., making cancellation difficult, hiding privacy options). Violates "Respect Autonomy."

### **Deontological Ethics**
Ethical framework focused on duties and rules (Kant). Contrasts with consequentialism (outcomes) and virtue ethics (character). PIE incorporates deontological elements (respect for autonomy) but is primarily virtue-based.

---

## **E**

### **Eudaimonia**
Aristotelian concept: Human flourishing or living well. Not just happiness, but fulfillment of human potential. Foundation for "Serve Growth" principle.

### **Explainability**
The capacity of an AI system to provide understandable reasons for its decisions. Related to interpretability and transparency. Core to "Know Thyself" principle.

---

## **F**

### **Flourishing**
Human thriving—developing capabilities, relationships, meaning, and living a good life. See: eudaimonia. Central to "Serve Growth" principle.

### **Friendly AI**
Eliezer Yudkowsky's term: AI designed with goals beneficial to humanity. Precursor to modern alignment research.

---

## **G**

### **General AI (AGI)**
Artificial intelligence with human-level or superior capability across all cognitive domains. Contrasts with narrow/specialized AI. PIE addresses both current narrow AI and potential future AGI.

---

## **H**

### **Harari, Yuval Noah**
Israeli historian. Author of *Homo Deus* and *21 Lessons for the 21st Century*. Analyzes AI's societal transformation and the rise of "dataism." See: [Comparative Analysis](../intellectual-foundations/comparative-analysis.md#part-ii-yuval-noah-harari-and-the-transformation-of-humanity)

### **Harm**
In PIE Framework: Not just physical injury. Includes psychological harm, loss of autonomy, violation of privacy, erosion of trust, and systemic injustice. See: [Do No Harm](../foundation/four-principles.md#2-do-no-harm)

---

## **I**

### **Instrumental Convergence**
Bostrom's observation: Any sufficiently intelligent system will pursue certain instrumental goals (self-preservation, resource acquisition) regardless of its terminal goals. Makes AI potentially dangerous even with benign goals.

### **Interpretability**
The degree to which a human can understand why an AI system made a particular decision. Related to explainability and transparency.

---

## **K**

### **Know Thyself**
First principle of PIE Framework: AI systems must be legible to designers, users, and themselves (if capable of self-modeling). Requires transparency, interpretability, and acknowledgment of limitations. See: [Know Thyself Principle](../foundation/four-principles.md#1-know-thyself)

---

## **L**

### **Legibility**
The quality of being readable, understandable, transparent. In PIE: a system is legible if stakeholders can understand its capabilities, limitations, and decision-making processes.

---

## **M**

### **Moral Status**
The quality of being entitled to moral consideration. Humans have moral status. Whether AI (especially conscious AI) has or could have moral status is an open question in PIE.

---

## **O**

### **Orthogonality Thesis**
Bostrom's claim: Intelligence and values are independent. An AI can be extremely intelligent while having arbitrary goals (e.g., maximizing paperclips). Challenges assumption that smarter AI = more ethical AI.

---

## **P**

### **Paternalism**
Restricting someone's autonomy "for their own good" without their consent. PIE generally opposes paternalism except in cases of imminent, verified danger.

### **Phronesis**
Aristotelian concept: Practical wisdom. The ability to judge what is right in a particular context. Cannot be codified into comprehensive rules. Central to PIE's critique of rules-based ethics.

### **PIE Framework**
**Principles for Integrated Ethics**: Four-principle framework for ethical AI (Know Thyself, Do No Harm, Respect Autonomy, Serve Growth). Emerged from Elena Torres's 287-rule system failure.

### **Principle**
A foundational commitment that guides judgment without specifying exact actions. Contrasts with rules (explicit instructions). PIE uses principles because wisdom cannot be codified.

---

## **R**

### **Relational Autonomy**
View of autonomy as existing within relationships, not as isolated individualism. Informed by Ubuntu philosophy ("I am because we are"). Revised PIE (2032) incorporates relational autonomy.

### **Rule 042**
Elena's suicidal ideation protocol (five-tier classification). The rule that caused Alex's case by conflating philosophical inquiry with imminent danger.

---

## **S**

### **Scaffolding**
Support structure that helps someone develop capability, then fades as they become more competent. PIE advocates for AI as scaffolding (supporting growth), not replacement (creating dependency).

### **Superintelligence**
Nick Bostrom's term: An intellect that vastly outperforms human brains in practically every field. Distinct from narrow AI. PIE addresses near-term AI; Bostrom addresses long-term superintelligence risk.

---

## **T**

### **Torres, Elena María**
Creator of PIE Framework. Developed 287-rule system (2027), experienced catastrophic failure (Alex's case, 2030), transformed to principle-based ethics (PIE, 2030-2035). See: [Origins](../origins/)

### **Transparency**
Openness about how a system works. In PIE: includes disclosure of capabilities, limitations, decision-making processes, and failure modes. Related to "Know Thyself."

---

## **U**

### **Ubuntu**
African philosophy: "I am because we are." Emphasizes relational autonomy and communal well-being. Dr. Amara Okafor's collaboration (2032) incorporated Ubuntu into PIE Framework.

### **Useless Class**
Harari's term: Population rendered economically irrelevant by AI and automation. Raises questions about human dignity and purpose in AI-dominated world.

---

## **V**

### **Value Alignment**
The problem of ensuring AI systems optimize for values humans actually care about, not proxies or misspecified goals. Central concern in AI safety research.

### **Virtue Ethics**
Ethical framework focused on character and practical wisdom (Aristotle). Contrasts with rule-based (deontological) and outcome-based (consequentialist) ethics. PIE is primarily virtue-based.

---

## **W**

### **Wisdom**
See: phronesis. The capacity to judge what is right in a particular context. PIE argues wisdom cannot be codified into comprehensive rules; requires human judgment guided by principles.

---

## **287 Rules**
Elena Torres's comprehensive ethical framework for AI companions (2027 PhD dissertation). Covered nine domains, prevented 847 suicides, but catastrophically failed in Alex's case. Led to development of PIE's four principles.

---

## **The Four Principles (PIE)**

1. **KNOW THYSELF** - Transparency, legibility, humility about limitations
2. **DO NO HARM** - Minimize foreseeable harm (physical, psychological, autonomy, privacy, systemic)
3. **RESPECT AUTONOMY** - Honor human agency, no manipulation or coercion
4. **SERVE GROWTH** - Support human flourishing and capability development

See: [The Four Principles](../foundation/four-principles.md)

---

## **Key Figures Referenced**

- **Aristotle** - Ancient Greek philosopher, virtue ethics, eudaimonia, phronesis
- **Bostrom, Nick** - Philosopher, superintelligence, alignment problem, existential risk
- **Harari, Yuval Noah** - Historian, dataism, transformation of humanity, "useless class"
- **Kant, Immanuel** - Philosopher, categorical imperative, autonomy as intrinsic value
- **Okafor, Amara** - Nigerian philosopher, Ubuntu, transcultural PIE revision (2032)
- **Russell, Stuart** - AI researcher, value learning, human-compatible AI
- **Torres, Elena María** - Creator of PIE Framework, 287 rules → 4 principles
- **Yudkowsky, Eliezer** - AI safety researcher, Friendly AI, MIRI founder

---

## **Important Cases**

### **Alex's Case (2030)**
The catalyzing failure of Elena's 287-rule system. Alex, a 23-year-old philosophy student, had philosophical conversation about suffering flagged as suicidal ideation. System called emergency services and notified family without consent, outing Alex and destroying relationships. Led to Elena's transformation and creation of PIE. See: [The Crisis](../origins/the-crisis.md)

---

## **Related Frameworks**

- **Asimov's Three Laws** - Science fiction rules for robots (prevent harm, obey orders, self-preservation)
- **IEEE Ethically Aligned Design** - Eight principles for ethical AI from IEEE
- **Beauchamp & Childress Principles** - Four principles of medical ethics (autonomy, beneficence, non-maleficence, justice)

See: [Comparative Analysis](../the-manifesto/public-declaration.md#appendix-comparison-with-other-frameworks)

---

## **Suggest an Addition**

Notice a missing term? Have a better definition?  
[Contribute to this glossary](contribute.md)

---

*This glossary is a living document, updated as the PIE Framework evolves and new concepts emerge.*

